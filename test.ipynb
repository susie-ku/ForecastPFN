{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "from keras import backend\n",
    "\n",
    "# from data_provider.data_factory import data_provider\n",
    "# from ..utils.metrics import smape\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\" Calculate Armstrong's original definition of sMAPE between `y_true` & `y_pred`.\n",
    "        `loss = 200 * mean(abs((y_true - y_pred) / (y_true + y_pred), axis=-1)`\n",
    "        Args:\n",
    "        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
    "        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
    "        Returns:\n",
    "        Symmetric mean absolute percentage error values. shape = `[batch_size, d0, ..\n",
    "        dN-1]`.\n",
    "        \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    diff = tf.abs(\n",
    "        (y_true - y_pred) /\n",
    "        backend.maximum(y_true + y_pred, backend.epsilon())\n",
    "    )\n",
    "    return 200.0 * backend.mean(diff, axis=-1)\n",
    "\n",
    "def smapeofa(true, pred, agg=np.mean):\n",
    "    smape = 200 * np.abs(pred - true) / (np.abs(pred) + np.abs(true) + 1e-8)\n",
    "    if agg is None:\n",
    "        return smape\n",
    "    else:\n",
    "        return agg(smape)\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "class M4ZeroShotDataset(Dataset):   \n",
    "    def __init__(self, train, horizon, history, test=None, max_size=500):\n",
    "        self.make_test = False\n",
    "        if test is None:\n",
    "            self.make_test = True\n",
    "        else:\n",
    "            self.test = self.load_data(test)\n",
    "\n",
    "        self.train = self.load_data(train)\n",
    "\n",
    "        self.horizon = horizon\n",
    "        self.history = history\n",
    "        self.max_size = max_size\n",
    "\n",
    "\n",
    "    def load_data(self, data):\n",
    "        if isinstance(data, list):\n",
    "            timeseries = data\n",
    "        else:\n",
    "            if isinstance(data, str):\n",
    "                if data.split('.')[-1] == 'tsf':\n",
    "                    data = convert_tsf_to_dataframe(data)[0]\n",
    "                    timeseries = [self.dropna(ts).astype(np.float32) for ts in data.loc[:, 'series_value'].values]\n",
    "                else:\n",
    "                    data = pd.read_csv(data) \n",
    "                    timeseries = [self.dropna(ts).values.astype(np.float32) for n, ts in data.iloc[:, 1:].iterrows()]\n",
    "        return timeseries\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dropna(x):\n",
    "        return x[~np.isnan(x)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        series_train = self.train[index]\n",
    "        if self.make_test:\n",
    "            series_test = series_train[-self.horizon:]\n",
    "            series_train = series_train[:-self.horizon]\n",
    "        else:\n",
    "            series_test = self.test[index]\n",
    "\n",
    "        len_train = np.minimum(self.history, len(series_train))\n",
    "        pad_size = self.max_size - len_train\n",
    "\n",
    "        x_train = series_train[-len_train:]\n",
    "        x_val = series_test\n",
    "\n",
    "        train_min = np.min(x_train)\n",
    "        train_ptp = np.ptp(x_train)\n",
    "        if np.isclose(train_ptp, 0):\n",
    "            train_ptp = 1\n",
    "        x_train = (x_train - train_min) / train_ptp\n",
    "        x_val = (x_val - train_min) / train_ptp\n",
    "        x_train = x_train.astype(np.float32)\n",
    "        x_val = x_val.astype(np.float32)\n",
    "\n",
    "        mask_train = np.pad(np.ones(len(x_train)), (0, pad_size), mode = 'constant')\n",
    "        x_train = np.pad(x_train, (0, pad_size), mode = 'constant')\n",
    "        \n",
    "        \n",
    "        out = {\n",
    "            'series': x_train.astype('float32'),\n",
    "            'target': x_val.astype('float32'),\n",
    "            'mask_series': mask_train,\n",
    "            'min': train_min,\n",
    "            'ptp': train_ptp,\n",
    "        }\n",
    "        return out\n",
    "    \n",
    "def sliding_window_view(a, window):\n",
    "    \"\"\"Generate window view.\"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1,) + (window,)\n",
    "    strides = a.strides[:-1] + (a.strides[-1],) + a.strides[-1:]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def rolling_window(a, window, step=1, from_last=True):\n",
    "    \"\"\"from_last == True - will cut first step-1 elements\"\"\"\n",
    "    sliding_window = (\n",
    "        sliding_window_view(a, window)\n",
    "        if np.__version__ < \"1.20\"\n",
    "        else np.lib.stride_tricks.sliding_window_view(a, window)\n",
    "    )\n",
    "    return sliding_window[(len(a) - window) % step if from_last else 0 :][::step]\n",
    "\n",
    "\n",
    "class TopInd:\n",
    "    def __init__(\n",
    "        self, n_target=7, history=100, step=3, from_last=True, test_last=True, date_col=None, scheme=None, **kwargs\n",
    "    ):\n",
    "        self.n_target = n_target\n",
    "        self.history = history\n",
    "        self.step = step\n",
    "        self.from_last = from_last\n",
    "        self.test_last = test_last\n",
    "        self.date_col = date_col\n",
    "\n",
    "    @staticmethod\n",
    "    def _timedelta(x):\n",
    "        delta = pd.to_datetime(x).diff().iloc[-1]\n",
    "        if delta <= pd.Timedelta(days=1):\n",
    "            return pd.to_datetime(x).diff().fillna(delta).values, delta\n",
    "\n",
    "        if delta > pd.Timedelta(days=360):\n",
    "            d = pd.to_datetime(x).dt.year.diff()\n",
    "            delta = d.iloc[-1]\n",
    "            return d.fillna(delta).values, delta\n",
    "        elif delta > pd.Timedelta(days=27):\n",
    "            d = pd.to_datetime(x).dt.month.diff() + 12 * pd.to_datetime(x).dt.year.diff()\n",
    "            delta = d.iloc[-1]\n",
    "            return d.fillna(delta).values, delta\n",
    "        else:\n",
    "            return pd.to_datetime(x).diff().fillna(delta).values, delta\n",
    "\n",
    "    def read(self, data, plain_data=None):\n",
    "        self.len_data = len(data)\n",
    "        self.date_col = self.date_col\n",
    "        self.time_delta = self._timedelta(data[self.date_col])[1]\n",
    "        return self\n",
    "        # TODO: add asserts\n",
    "\n",
    "    def _create_test(self, data=None, plain_data=None):\n",
    "        # for predicting future\n",
    "        return rolling_window(\n",
    "            np.arange(self.len_data if data is None else len(data)), self.history, self.step, self.from_last\n",
    "        )[-1 if self.test_last else 0 :, :]\n",
    "\n",
    "    def _create_data(self, data=None, plain_data=None):\n",
    "\n",
    "        return rolling_window(\n",
    "            np.arange(self.len_data if data is None else len(data))[: -self.n_target],\n",
    "            self.history,\n",
    "            self.step,\n",
    "            self.from_last,\n",
    "        )\n",
    "\n",
    "    def _create_target(self, data=None, plain_data=None):\n",
    "        return rolling_window(\n",
    "            np.arange(self.len_data if data is None else len(data))[self.history :],\n",
    "            self.n_target,\n",
    "            self.step,\n",
    "            self.from_last,\n",
    "        )\n",
    "\n",
    "    def _get_ids(self, data=None, plain_data=None, func=None, cond=None):\n",
    "        date_col = pd.to_datetime(data[self.date_col])\n",
    "        vals, time_delta = self._timedelta(data[self.date_col])\n",
    "        ids = list(np.argwhere(vals != time_delta).flatten())\n",
    "        prev = 0\n",
    "        inds = []\n",
    "        for split in ids + [len(date_col)]:\n",
    "            segment = date_col.iloc[prev:split]\n",
    "            if len(segment) > cond:\n",
    "                ind = func(segment) + prev\n",
    "                inds.append(ind)\n",
    "            prev = split\n",
    "        inds = np.vstack(inds)\n",
    "        return inds\n",
    "\n",
    "    def create_data(self, data=None, plain_data=None):\n",
    "        return self._get_ids(data, plain_data, self._create_data, self.n_target + self.history)\n",
    "\n",
    "    def create_test(self, data=None, plain_data=None):\n",
    "        return self._get_ids(data, plain_data, self._create_test, self.history)\n",
    "\n",
    "    def create_target(self, data=None, plain_data=None):\n",
    "        return self._get_ids(data, plain_data, self._create_target, self.n_target + self.history)\n",
    "\n",
    "class BenchZeroShotDataset(Dataset):   \n",
    "    def __init__(self, horizon, history, real_size, train, borders, max_size=500, date_column = 'date', scale=True):\n",
    "\n",
    "        self.horizon = horizon\n",
    "        self.history = history\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.real_size = real_size\n",
    "        self.borders = borders\n",
    "\n",
    "        self.date_column = date_column\n",
    "        self.scale = scale\n",
    "        self.scaler = None\n",
    "        self.targets = None\n",
    "\n",
    "        train, val, test = self.load_data(train)\n",
    "\n",
    "        L_split_data = val[date_column].values[(len(val) - history) if (len(val) - history) > 0 else 0]\n",
    "        L_last_val_data = val[val[date_column] >= L_split_data]\n",
    "        if (len(val) - history) < 0:\n",
    "            L_split_data = train[date_column].values[(len(train) - (history- len(L_last_val_data))) if (len(train) - (history- len(L_last_val_data))) > 0 else 0]\n",
    "            L_last_train_data = train[train[date_column] >= L_split_data]\n",
    "            test_data_expanded = pd.concat((L_last_train_data, L_last_val_data, test))\n",
    "        else:\n",
    "            test_data_expanded = pd.concat((L_last_val_data, test))\n",
    "        test_data_expanded = test_data_expanded.sort_values([date_column]).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.data = test_data_expanded\n",
    "        slicer = TopInd(n_target=horizon, history=history, step=1, from_last=False, test_last=False, date_col=date_column)\n",
    "        slicer.read(self.data)\n",
    "\n",
    "        self.ids = slicer.create_data(self.data)\n",
    "        self.ids_y = slicer.create_target(self.data)\n",
    "        \n",
    "        self.data = self.data.loc[:, self.targets].values\n",
    "        \n",
    "        # add time features\n",
    "        # self.ts = self.data.loc[:, self.data_stamp].values\n",
    "        # self.ts = self._ForecastPFN_time_features(self.data.loc[:, self.data_stamp].values)\n",
    "        # self.data_stamp_original = df_raw[border1:border2]\n",
    "        # test_data.data_stamp = self._ForecastPFN_time_features(list(test_data.data_stamp_original['date']))\n",
    "        \n",
    "    def _ForecastPFN_time_features(self, ts: np.ndarray):\n",
    "        if type(ts[0]) == datetime:\n",
    "            year = [x.year for x in ts]\n",
    "            month = [x.month for x in ts]\n",
    "            day = [x.day for x in ts]\n",
    "            day_of_week = [x.weekday()+1 for x in ts]\n",
    "            day_of_year = [x.timetuple().tm_yday for x in ts]\n",
    "            return np.stack([year, month, day, day_of_week, day_of_year], axis=-1)\n",
    "        ts = pd.to_datetime(ts)\n",
    "        return np.stack([ts.year, ts.month, ts.day, ts.day_of_week + 1, ts.day_of_year], axis=-1)\n",
    "\n",
    "\n",
    "    def load_data(self, data):\n",
    "        data = pd.read_csv(data)\n",
    "        \n",
    "        self.targets = list(data.columns.drop(self.date_column))\n",
    "\n",
    "        if self.borders is None:\n",
    "            TEST_SIZE = 0.2\n",
    "            VAL_SIZE = 0.1\n",
    "\n",
    "            train_val_split_data = data[self.date_column].values[\n",
    "                int(data[self.date_column].nunique() * (1 - (VAL_SIZE + TEST_SIZE)))\n",
    "            ]\n",
    "            val_test_slit_data = data[self.date_column].values[\n",
    "                int(data[self.date_column].nunique() * (1 - TEST_SIZE))\n",
    "            ]\n",
    "\n",
    "            train = data[data[self.date_column] <= train_val_split_data]\n",
    "            val = data[\n",
    "                (data[self.date_column] > train_val_split_data) & (data[self.date_column] <= val_test_slit_data)\n",
    "            ]\n",
    "            test = data[data[self.date_column] > val_test_slit_data]\n",
    "\n",
    "        else:\n",
    "            train = data.iloc[:self.borders[0]]\n",
    "            val = data.iloc[self.borders[0]:self.borders[1]]\n",
    "            test = data.iloc[self.borders[1]:self.borders[2]]\n",
    "\n",
    "        if self.scale:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(train.loc[:, self.targets].values)\n",
    "            train.loc[:, self.targets] = self.scaler.transform(train.loc[:, self.targets].values)\n",
    "            val.loc[:, self.targets] = self.scaler.transform(val.loc[:, self.targets].values)\n",
    "            test.loc[:, self.targets] = self.scaler.transform(test.loc[:, self.targets].values)\n",
    "        \n",
    "        self.train_ts = self._ForecastPFN_time_features(list(train[self.date_column]))\n",
    "        self.val_ts = self._ForecastPFN_time_features(list(val[self.date_column]))\n",
    "        self.test_ts = self._ForecastPFN_time_features(list(test[self.date_column]))\n",
    "        \n",
    "\n",
    "        return train, val, test\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.ids) * len(self.targets))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #index = index + 1\n",
    "        ind, target = index // len(self.targets), index % len(self.targets)\n",
    "        series_train = self.data[self.ids[ind], target]\n",
    "        series_test = self.data[self.ids_y[ind], target]\n",
    "        \n",
    "        seq_x_mark = self.train_ts[self.ids[ind]]\n",
    "        seq_y_mark = self.test_ts[self.ids_y[ind]]\n",
    "\n",
    "        len_train = np.minimum(self.real_size, len(series_train))\n",
    "        pad_size = self.max_size - len_train\n",
    "\n",
    "        x_train = series_train[-len_train:].astype(np.float32)\n",
    "        x_val = series_test.astype(np.float32)\n",
    "        seq_x_mark = seq_x_mark.astype(np.float32)\n",
    "        seq_y_mark = seq_y_mark.astype(np.float32)\n",
    "        \n",
    "        if np.all(x_train == x_train[0]):\n",
    "            x_train[-1] += 1\n",
    "        history = x_train.copy()\n",
    "        mean = np.nanmean(history)\n",
    "        std = np.nanstd(history)\n",
    "        history = (history - mean) / std\n",
    "        history_mean = np.nanmean(history[-6:])\n",
    "        history_std = np.nanstd(history[-6:])\n",
    "        local_scale = (history_mean + history_std + 1e-4)\n",
    "        history = np.clip(history / local_scale, a_min=0, a_max=1)\n",
    "#         print(x_train.shape[0])\n",
    "        \n",
    "        if x_train.shape[0] != 100:\n",
    "            seq_x_mark = seq_x_mark.astype(np.int64)\n",
    "            if x_train.shape[0] > 100:\n",
    "                target = seq_x_mark[-100:, :]\n",
    "                history = history[-100:, :]\n",
    "            else:\n",
    "                target = np.pad(seq_x_mark, (0, 100-x_train.shape[0]))\n",
    "                history = np.pad(history, (0, 100-x_train.shape[0]))\n",
    "                \n",
    "#             history = np.repeat(np.expand_dims(history, axis=0), self.horizon, axis=0)[:, :, 0]\n",
    "#             history = np.expand_dims(history, axis=0)\n",
    "            ts = np.repeat(np.expand_dims(target, axis=0), self.horizon, axis=0)\n",
    "\n",
    "        else:\n",
    "            ts = np.repeat(np.expand_dims(seq_x_mark, axis=0), self.horizon, axis=1).astype(np.int64)\n",
    "            history = history.astype(np.float32)\n",
    "            \n",
    "#         task = np.full((self.horizon, ), 1)  \n",
    "        task = 1\n",
    "#         target_ts = np.expand_dims(seq_y_mark[-self.horizon:, :], axis=1).astype(np.int64)\n",
    "#         print(seq_y_mark.shape)\n",
    "        target_ts = seq_y_mark[-self.horizon:, :].astype(np.int64)\n",
    "#         print(seq_y_mark.shape)\n",
    "        model_input = {'ts': ts, 'history': history, 'target_ts': target_ts, 'task': task}\n",
    "\n",
    "        return model_input, mean, std\n",
    "        \n",
    "        \n",
    "def test(model, loader, device):\n",
    "    trues = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            model_input = batch[0]\n",
    "            mean = batch[1]\n",
    "            std = batch[2]\n",
    "            pred_vals = model(model_input).data.cpu()\n",
    "            scaled_vals = pred_vals['result'].numpy().T.reshape(-1) * pred_vals['scale'].numpy().reshape(-1)\n",
    "            scaled_vals = scaled_vals * std + mean\n",
    "            true_vals = model_input['history']\n",
    "#             print(scaled_vals.shape)\n",
    "#             print(true_vals.shape)\n",
    "\n",
    "            trues.append(list(scaled_vals.numpy()))\n",
    "            preds.append(list(true_vals.numpy()))\n",
    "              \n",
    "    trues = np.vstack(trues)\n",
    "    preds = np.vstack(preds)\n",
    "    return preds, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ts': array([[[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "  \n",
       "         [[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "  \n",
       "         [[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "  \n",
       "         [[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "  \n",
       "         [[2002,    1,    1, ...,    0,    0,    0],\n",
       "          [2002,    1,    8, ...,    0,    0,    0],\n",
       "          [2002,    1,   15, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]),\n",
       "  'history': array([0.47890085, 0.29518476, 0.36141187, 0.29408908, 0.08902771,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2552751 , 0.41885266, 0.5377078 , 0.6418889 ,\n",
       "         0.7375056 , 0.73162496, 0.8126024 , 0.8520335 , 0.8981081 ,\n",
       "         0.9939321 , 0.9956195 , 1.        , 1.        , 1.        ,\n",
       "         1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        dtype=float32),\n",
       "  'target_ts': array([[2017,    7,    4,    2,  185],\n",
       "         [2017,    7,   11,    2,  192],\n",
       "         [2017,    7,   18,    2,  199],\n",
       "         [2017,    7,   25,    2,  206],\n",
       "         [2017,    8,    1,    2,  213],\n",
       "         [2017,    8,    8,    2,  220],\n",
       "         [2017,    8,   15,    2,  227],\n",
       "         [2017,    8,   22,    2,  234],\n",
       "         [2017,    8,   29,    2,  241],\n",
       "         [2017,    9,    5,    2,  248],\n",
       "         [2017,    9,   12,    2,  255],\n",
       "         [2017,    9,   19,    2,  262],\n",
       "         [2017,    9,   26,    2,  269],\n",
       "         [2017,   10,    3,    2,  276],\n",
       "         [2017,   10,   10,    2,  283],\n",
       "         [2017,   10,   17,    2,  290],\n",
       "         [2017,   10,   24,    2,  297],\n",
       "         [2017,   10,   31,    2,  304],\n",
       "         [2017,   11,    7,    2,  311],\n",
       "         [2017,   11,   14,    2,  318],\n",
       "         [2017,   11,   21,    2,  325],\n",
       "         [2017,   11,   28,    2,  332],\n",
       "         [2017,   12,    5,    2,  339],\n",
       "         [2017,   12,   12,    2,  346]]),\n",
       "  'task': 1},\n",
       " 0.4883957,\n",
       " 1.02852)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LEN = 36\n",
    "PATH = 'academic_data'\n",
    "config = {\n",
    "    'horizon': 24, \n",
    "    'history': 36, \n",
    "    'real_size': SEQ_LEN, \n",
    "    'train': f'{PATH}/illness/national_illness.csv', \n",
    "    'borders': None, \n",
    "    'max_size':500\n",
    "}\n",
    "dataset = BenchZeroShotDataset(**config)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transformer_model\" (type TransformerModel).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * {'history': <tf.Tensor 'x:0' shape=(512, 100) dtype=float32>,\n 'target_ts': <tf.Tensor 'x_1:0' shape=(512, 24, 5) dtype=int64>,\n 'task': <tf.Tensor 'x_2:0' shape=(512,) dtype=int64>,\n 'ts': <tf.Tensor 'x_3:0' shape=(512, 24, 100, 69) dtype=int64>}\n    * False\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='x/history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='x/target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='x/task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='x/ts')}\n    * False\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='x/history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='x/target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='x/task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='x/ts')}\n    * True\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='ts')}\n    * False\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='ts')}\n    * True\n  Keyword arguments: {}\n\nCall arguments received by layer \"transformer_model\" (type TransformerModel):\n  • args=({'ts': 'tensor([[[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        ...,\\n\\n\\n        [[[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]]])', 'history': 'tensor([[0.4789, 0.2952, 0.3614,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.4591, 0.2313, 0.3453,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\\n        ...,\\n        [0.1190, 0.0532, 0.0950,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.4193, 0.2732, 0.2052,  ..., 0.0000, 0.0000, 0.0000]])', 'target_ts': 'tensor([[[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        [[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        [[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        ...,\\n\\n        [[2018,   11,   20,    2,  324],\\n         [2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         ...,\\n         [2019,    4,   16,    2,  106],\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120]],\\n\\n        [[2018,   11,   20,    2,  324],\\n         [2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         ...,\\n         [2019,    4,   16,    2,  106],\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120]],\\n\\n        [[2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         [2018,   12,   11,    2,  345],\\n         ...,\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120],\\n         [2019,    5,    7,    2,  127]]])', 'task': 'tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1])'},)\n  • kwargs={'training': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     18\u001b[0m     dataset,\n\u001b[1;32m     19\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m pretrained \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_weights/\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmape\u001b[39m\u001b[38;5;124m'\u001b[39m: smape})\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 531\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    529\u001b[0m mean \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    530\u001b[0m std \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 531\u001b[0m pred_vals \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    532\u001b[0m scaled_vals \u001b[38;5;241m=\u001b[39m pred_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m pred_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    533\u001b[0m scaled_vals \u001b[38;5;241m=\u001b[39m scaled_vals \u001b[38;5;241m*\u001b[39m std \u001b[38;5;241m+\u001b[39m mean\n",
      "File \u001b[0;32m~/anaconda3/envs/fpfn/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/fpfn/lib/python3.9/site-packages/tensorflow/python/saved_model/function_deserialization.py:286\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m   positional, keyword \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mstructured_input_signature\n\u001b[1;32m    283\u001b[0m   signature_descriptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    284\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOption \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword arguments: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel. Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_pretty_format_positional(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Expected these arguments to match one of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(saved_function\u001b[38;5;241m.\u001b[39mconcrete_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mjoin(signature_descriptions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"transformer_model\" (type TransformerModel).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (2 total):\n    * {'history': <tf.Tensor 'x:0' shape=(512, 100) dtype=float32>,\n 'target_ts': <tf.Tensor 'x_1:0' shape=(512, 24, 5) dtype=int64>,\n 'task': <tf.Tensor 'x_2:0' shape=(512,) dtype=int64>,\n 'ts': <tf.Tensor 'x_3:0' shape=(512, 24, 100, 69) dtype=int64>}\n    * False\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='x/history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='x/target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='x/task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='x/ts')}\n    * False\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='x/history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='x/target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='x/task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='x/ts')}\n    * True\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='ts')}\n    * False\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (2 total):\n    * {'history': TensorSpec(shape=(None, 100), dtype=tf.float32, name='history'),\n 'target_ts': TensorSpec(shape=(None, 1, 5), dtype=tf.int64, name='target_ts'),\n 'task': TensorSpec(shape=(None,), dtype=tf.int32, name='task'),\n 'ts': TensorSpec(shape=(None, 100, 5), dtype=tf.int64, name='ts')}\n    * True\n  Keyword arguments: {}\n\nCall arguments received by layer \"transformer_model\" (type TransformerModel):\n  • args=({'ts': 'tensor([[[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2002,    1,    1,  ...,    0,    0,    0],\\n          [2002,    1,    8,  ...,    0,    0,    0],\\n          [2002,    1,   15,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        ...,\\n\\n\\n        [[[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   20,  ...,    0,    0,    0],\\n          [2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]],\\n\\n\\n        [[[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         ...,\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]],\\n\\n         [[2003,    5,   27,  ...,    0,    0,    0],\\n          [2003,    6,    3,  ...,    0,    0,    0],\\n          [2003,    6,   10,  ...,    0,    0,    0],\\n          ...,\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0],\\n          [   0,    0,    0,  ...,    0,    0,    0]]]])', 'history': 'tensor([[0.4789, 0.2952, 0.3614,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.4591, 0.2313, 0.3453,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\\n        ...,\\n        [0.1190, 0.0532, 0.0950,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\\n        [0.4193, 0.2732, 0.2052,  ..., 0.0000, 0.0000, 0.0000]])', 'target_ts': 'tensor([[[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        [[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        [[2017,    7,    4,    2,  185],\\n         [2017,    7,   11,    2,  192],\\n         [2017,    7,   18,    2,  199],\\n         ...,\\n         [2017,   11,   28,    2,  332],\\n         [2017,   12,    5,    2,  339],\\n         [2017,   12,   12,    2,  346]],\\n\\n        ...,\\n\\n        [[2018,   11,   20,    2,  324],\\n         [2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         ...,\\n         [2019,    4,   16,    2,  106],\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120]],\\n\\n        [[2018,   11,   20,    2,  324],\\n         [2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         ...,\\n         [2019,    4,   16,    2,  106],\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120]],\\n\\n        [[2018,   11,   27,    2,  331],\\n         [2018,   12,    4,    2,  338],\\n         [2018,   12,   11,    2,  345],\\n         ...,\\n         [2019,    4,   23,    2,  113],\\n         [2019,    4,   30,    2,  120],\\n         [2019,    5,    7,    2,  127]]])', 'task': 'tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n        1, 1, 1, 1, 1, 1, 1, 1])'},)\n  • kwargs={'training': 'False'}"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 36\n",
    "PATH = 'academic_data'\n",
    "    # configs = { \n",
    "    #     'Weather_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/weather/weather.csv', 'borders': None, 'max_size':500}, \n",
    "    #     #'Traffic_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/traffic/traffic.csv', 'borders': None, 'max_size':500}, \n",
    "    #     #'Electricity_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/electricity/electricity.csv', 'borders': None, 'max_size':500}, \n",
    "    #     'ILI_24_148': {'horizon': 24, 'history': 148, 'real_size': SEQ_LEN, 'train': f'{PATH}/illness/national_illness.csv', 'borders': None, 'max_size':500}, \n",
    "    #     'ETTh1_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/ETT-small/ETTh1.csv', 'borders': [8640, 11520, 14400], 'max_size':500}, \n",
    "    #     'ETTh2_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/ETT-small/ETTh2.csv', 'borders': [8640, 11520, 14400], 'max_size':500}, \n",
    "    #     'ETTm1_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/ETT-small/ETTm1.csv', 'borders': [34560, 46080, 57600], 'max_size':500}, \n",
    "    #     'ETTm2_96_250': {'horizon': 96, 'history': 512, 'real_size': SEQ_LEN, 'train': f'{PATH}/ETT-small/ETTm2.csv', 'borders': [34560, 46080, 57600], 'max_size':500},\n",
    "    #     'Demand': {'horizon': 30, 'history': 50, 'real_size': SEQ_LEN, 'train': '/media/ssd-3t/Kostromina/pyboost_experiments/data/demand/demand_scaled_transformer_format.csv', 'borders': None, 'max_size':500, 'scale': False}, \n",
    "    # }\n",
    "config = {'horizon': 24, 'history': 36, 'real_size': SEQ_LEN, 'train': f'{PATH}/illness/national_illness.csv', 'borders': None, 'max_size':500}\n",
    "device = 'cuda'\n",
    "dataset = BenchZeroShotDataset(**config)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")\n",
    "pretrained = tf.keras.models.load_model(\"saved_weights/\", custom_objects={'smape': smape})\n",
    "test(pretrained, loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = Dataset_Custom(root_path='/home/jovyan/kkuvshinova/ForecastPFN_accelerate/academic_data/ETT-small', flag='test', timeenc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_len': 384,\n",
       " 'label_len': 96,\n",
       " 'pred_len': 96,\n",
       " 'set_type': 2,\n",
       " 'features': 'S',\n",
       " 'target': 'OT',\n",
       " 'scale': True,\n",
       " 'timeenc': 1,\n",
       " 'freq': 'h',\n",
       " 'scaler': StandardScaler(),\n",
       " 'train_budget': None,\n",
       " 'root_path': '/home/jovyan/kkuvshinova/ForecastPFN_accelerate/academic_data/ETT-small',\n",
       " 'data_path': 'ETTh1_.csv',\n",
       " 'data_stamp_original':                       date  Unnamed: 0    HUFL   HULL   MUFL   MULL   LUFL  \\\n",
       " 13552  2018-01-16 16:00:00       13552   9.578  2.009  5.899  0.533  3.686   \n",
       " 13553  2018-01-16 17:00:00       13553  12.257  2.679  7.995  0.817  4.447   \n",
       " 13554  2018-01-16 18:00:00       13554  11.855  2.210  7.569  0.746  4.295   \n",
       " 13555  2018-01-16 19:00:00       13555  11.588  2.076  7.427  0.569  4.142   \n",
       " 13556  2018-01-16 20:00:00       13556  10.784  2.210  6.752  0.569  4.112   \n",
       " ...                    ...         ...     ...    ...    ...    ...    ...   \n",
       " 17415  2018-06-26 15:00:00       17415  -1.674  3.550 -5.615  2.132  3.472   \n",
       " 17416  2018-06-26 16:00:00       17416  -5.492  4.287 -9.132  2.274  3.533   \n",
       " 17417  2018-06-26 17:00:00       17417   2.813  3.818 -0.817  2.097  3.716   \n",
       " 17418  2018-06-26 18:00:00       17418   9.243  3.818  5.472  2.097  3.655   \n",
       " 17419  2018-06-26 19:00:00       17419  10.114  3.550  6.183  1.564  3.716   \n",
       " \n",
       "         LULL      OT  \n",
       " 13552  1.097   4.080  \n",
       " 13553  1.036   3.588  \n",
       " 13554  0.975   3.588  \n",
       " 13555  0.944   4.502  \n",
       " 13556  1.097   4.713  \n",
       " ...      ...     ...  \n",
       " 17415  1.523  10.904  \n",
       " 17416  1.675  11.044  \n",
       " 17417  1.523  10.271  \n",
       " 17418  1.432   9.778  \n",
       " 17419  1.462   9.567  \n",
       " \n",
       " [3868 rows x 9 columns],\n",
       " 'data_x': array([[-1.46310785],\n",
       "        [-1.52204077],\n",
       "        [-1.52204077],\n",
       "        ...,\n",
       "        [-0.72153502],\n",
       "        [-0.78058775],\n",
       "        [-0.80586177]]),\n",
       " 'data_y': array([[-1.46310785],\n",
       "        [-1.52204077],\n",
       "        [-1.52204077],\n",
       "        ...,\n",
       "        [-0.72153502],\n",
       "        [-0.78058775],\n",
       "        [-0.80586177]]),\n",
       " 'data_stamp': array([[ 0.19565217, -0.33333333,  0.        , -0.45890411],\n",
       "        [ 0.23913043, -0.33333333,  0.        , -0.45890411],\n",
       "        [ 0.2826087 , -0.33333333,  0.        , -0.45890411],\n",
       "        ...,\n",
       "        [ 0.23913043, -0.33333333,  0.33333333, -0.01780822],\n",
       "        [ 0.2826087 , -0.33333333,  0.33333333, -0.01780822],\n",
       "        [ 0.32608696, -0.33333333,  0.33333333, -0.01780822]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/kkuvshinova/ForecastPFN_accelerate/test.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkkuvshinova-fpfn-2gpu-0/home/jovyan/kkuvshinova/ForecastPFN_accelerate/test.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkkuvshinova-fpfn-2gpu-0/home/jovyan/kkuvshinova/ForecastPFN_accelerate/test.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tf\u001b[39m.\u001b[39mconvert_to_tensor(tf\u001b[39m.\u001b[39;49mones([\u001b[39m5\u001b[39;49m, \u001b[39m1\u001b[39;49m], tf\u001b[39m.\u001b[39;49mint32)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)\n",
      "File \u001b[0;32m/home/user/conda/envs/fpfn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.convert_to_tensor(tf.ones([5, 1], tf.int32).unsqueeze(0).repeat(5, 1, 1), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/kkuvshinova/ForecastPFN_accelerate/test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkkuvshinova-fpfn-2gpu-0/home/jovyan/kkuvshinova/ForecastPFN_accelerate/test.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39;49mones([\u001b[39m5\u001b[39;49m, \u001b[39m1\u001b[39;49m], tf\u001b[39m.\u001b[39;49mfloat64)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/home/user/conda/envs/fpfn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "tf.ones([5, 1], tf.float64).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-1.4631],\n",
       "          [-1.5220],\n",
       "          [-1.5220],\n",
       "          [-1.4126],\n",
       "          [-1.3873],\n",
       "          [-1.3788],\n",
       "          [-1.3030],\n",
       "          [-1.5305],\n",
       "          [-1.6064],\n",
       "          [-1.5389],\n",
       "          [-1.5137],\n",
       "          [-1.3620],\n",
       "          [-1.2693],\n",
       "          [-1.2271],\n",
       "          [-1.1597],\n",
       "          [-1.2356],\n",
       "          [-1.2946],\n",
       "          [-1.1850],\n",
       "          [-1.1177],\n",
       "          [-1.1008],\n",
       "          [-1.2102],\n",
       "          [-1.3198],\n",
       "          [-1.3620],\n",
       "          [-1.2609],\n",
       "          [-1.1766],\n",
       "          [-1.2524],\n",
       "          [-1.3451],\n",
       "          [-1.3704],\n",
       "          [-1.3282],\n",
       "          [-1.2777],\n",
       "          [-1.3451],\n",
       "          [-1.3535],\n",
       "          [-1.4041],\n",
       "          [-1.3873],\n",
       "          [-1.4041],\n",
       "          [-1.4293],\n",
       "          [-1.4631],\n",
       "          [-1.4378],\n",
       "          [-1.4884],\n",
       "          [-1.4378],\n",
       "          [-1.4378],\n",
       "          [-1.4378],\n",
       "          [-1.4547],\n",
       "          [-1.4378],\n",
       "          [-1.4968],\n",
       "          [-1.4378],\n",
       "          [-1.4126],\n",
       "          [-1.4462],\n",
       "          [-1.4126],\n",
       "          [-1.4041],\n",
       "          [-1.3873],\n",
       "          [-1.4968],\n",
       "          [-1.5053],\n",
       "          [-1.4884],\n",
       "          [-1.4209],\n",
       "          [-1.3957],\n",
       "          [-1.3535],\n",
       "          [-1.4041],\n",
       "          [-1.3535],\n",
       "          [-1.3620],\n",
       "          [-1.3367],\n",
       "          [-1.3115],\n",
       "          [-1.3115],\n",
       "          [-1.2440],\n",
       "          [-1.2356],\n",
       "          [-1.3788],\n",
       "          [-1.3282],\n",
       "          [-1.3030],\n",
       "          [-1.2019],\n",
       "          [-1.1597],\n",
       "          [-1.2356],\n",
       "          [-1.2019],\n",
       "          [-1.1682],\n",
       "          [-1.1177],\n",
       "          [-1.2271],\n",
       "          [-1.2271],\n",
       "          [-1.2188],\n",
       "          [-1.2356],\n",
       "          [-1.2440],\n",
       "          [-1.2019],\n",
       "          [-1.2693],\n",
       "          [-1.1850],\n",
       "          [-1.2188],\n",
       "          [-1.2440],\n",
       "          [-1.2946],\n",
       "          [-1.2188],\n",
       "          [-1.2440],\n",
       "          [-1.3030],\n",
       "          [-1.3282],\n",
       "          [-1.3367],\n",
       "          [-1.3873],\n",
       "          [-1.4126],\n",
       "          [-1.3620],\n",
       "          [-1.3620],\n",
       "          [-1.3535],\n",
       "          [-1.3873],\n",
       "          [-1.3535],\n",
       "          [-1.3704],\n",
       "          [-1.3282],\n",
       "          [-1.3115],\n",
       "          [-1.2693],\n",
       "          [-1.2609],\n",
       "          [-1.2356],\n",
       "          [-1.2777],\n",
       "          [-1.3030],\n",
       "          [-1.2609],\n",
       "          [-1.2693],\n",
       "          [-1.3115],\n",
       "          [-1.3620],\n",
       "          [-1.3788],\n",
       "          [-1.4041],\n",
       "          [-1.4547],\n",
       "          [-1.4126],\n",
       "          [-1.3704],\n",
       "          [-1.4126],\n",
       "          [-1.4041],\n",
       "          [-1.4378],\n",
       "          [-1.3451],\n",
       "          [-1.4884],\n",
       "          [-1.4547],\n",
       "          [-1.5137],\n",
       "          [-1.6906],\n",
       "          [-1.6400],\n",
       "          [-1.6485],\n",
       "          [-1.5642],\n",
       "          [-1.3873],\n",
       "          [-1.3788],\n",
       "          [-1.3198],\n",
       "          [-1.2524],\n",
       "          [-1.2609],\n",
       "          [-1.2862],\n",
       "          [-1.4126],\n",
       "          [-1.3873],\n",
       "          [-1.3535],\n",
       "          [-1.5305],\n",
       "          [-1.5137],\n",
       "          [-1.5220],\n",
       "          [-1.3535],\n",
       "          [-1.2946],\n",
       "          [-1.2609],\n",
       "          [-1.2188],\n",
       "          [-1.3451],\n",
       "          [-1.3451],\n",
       "          [-1.2946],\n",
       "          [-1.3115],\n",
       "          [-1.2693],\n",
       "          [-1.2946],\n",
       "          [-1.3198],\n",
       "          [-1.3115],\n",
       "          [-1.3620],\n",
       "          [-1.3873],\n",
       "          [-1.3535],\n",
       "          [-1.3451],\n",
       "          [-1.3788],\n",
       "          [-1.4209],\n",
       "          [-1.5642],\n",
       "          [-1.4209],\n",
       "          [-1.6738],\n",
       "          [-1.7496],\n",
       "          [-1.7075],\n",
       "          [-1.5053],\n",
       "          [-1.6400],\n",
       "          [-1.6316],\n",
       "          [-1.6485],\n",
       "          [-1.6316],\n",
       "          [-1.5895],\n",
       "          [-1.5726],\n",
       "          [-1.6653],\n",
       "          [-1.6064],\n",
       "          [-1.6653],\n",
       "          [-1.7075],\n",
       "          [-1.7075],\n",
       "          [-1.7075],\n",
       "          [-1.7327],\n",
       "          [-1.7158],\n",
       "          [-1.7075],\n",
       "          [-1.7580],\n",
       "          [-1.7749],\n",
       "          [-1.8002],\n",
       "          [-1.8422],\n",
       "          [-1.8422],\n",
       "          [-1.8169],\n",
       "          [-1.8591],\n",
       "          [-1.9013],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-2.0698],\n",
       "          [-2.1120],\n",
       "          [-2.0867],\n",
       "          [-2.0529],\n",
       "          [-1.9518],\n",
       "          [-2.0529],\n",
       "          [-2.1456],\n",
       "          [-2.2046],\n",
       "          [-2.1456],\n",
       "          [-2.1035],\n",
       "          [-2.1793],\n",
       "          [-2.1962],\n",
       "          [-2.2298],\n",
       "          [-2.1709],\n",
       "          [-2.1456],\n",
       "          [-2.2467],\n",
       "          [-2.2467],\n",
       "          [-2.2131],\n",
       "          [-2.2298],\n",
       "          [-2.2383],\n",
       "          [-2.2889],\n",
       "          [-2.2805],\n",
       "          [-2.3142],\n",
       "          [-2.2467],\n",
       "          [-2.3225],\n",
       "          [-2.2973],\n",
       "          [-2.2383],\n",
       "          [-2.3225],\n",
       "          [-2.3142],\n",
       "          [-2.3058],\n",
       "          [-2.1540],\n",
       "          [-2.1456],\n",
       "          [-2.1120],\n",
       "          [-2.1456],\n",
       "          [-2.1372],\n",
       "          [-2.1793],\n",
       "          [-2.1793],\n",
       "          [-2.1793],\n",
       "          [-2.1456],\n",
       "          [-2.0529],\n",
       "          [-2.1035],\n",
       "          [-2.0867],\n",
       "          [-2.0782],\n",
       "          [-1.9518],\n",
       "          [-2.1120],\n",
       "          [-2.0193],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.8929],\n",
       "          [-1.8507],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-2.0276],\n",
       "          [-2.0276],\n",
       "          [-2.0360],\n",
       "          [-2.1035],\n",
       "          [-2.1962],\n",
       "          [-2.2467],\n",
       "          [-2.3984],\n",
       "          [-2.3563],\n",
       "          [-2.3394],\n",
       "          [-2.3478],\n",
       "          [-2.3225],\n",
       "          [-2.2720],\n",
       "          [-2.1878],\n",
       "          [-2.2046],\n",
       "          [-2.0951],\n",
       "          [-2.0867],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-2.0445],\n",
       "          [-2.0445],\n",
       "          [-2.0614],\n",
       "          [-2.0529],\n",
       "          [-2.1120],\n",
       "          [-2.0614],\n",
       "          [-2.0614],\n",
       "          [-2.0951],\n",
       "          [-2.1120],\n",
       "          [-2.1287],\n",
       "          [-2.0614],\n",
       "          [-2.0951],\n",
       "          [-1.9518],\n",
       "          [-2.0698],\n",
       "          [-2.0698],\n",
       "          [-1.9518],\n",
       "          [-1.8760],\n",
       "          [-1.8254],\n",
       "          [-1.8676],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.8844],\n",
       "          [-1.8844],\n",
       "          [-1.8169],\n",
       "          [-1.8422],\n",
       "          [-1.8254],\n",
       "          [-1.7749],\n",
       "          [-1.8591],\n",
       "          [-1.8591],\n",
       "          [-1.7917],\n",
       "          [-1.8676],\n",
       "          [-1.8591],\n",
       "          [-1.8929],\n",
       "          [-1.8169],\n",
       "          [-1.7749],\n",
       "          [-1.7496],\n",
       "          [-1.7580],\n",
       "          [-1.7158],\n",
       "          [-1.7411],\n",
       "          [-1.7749],\n",
       "          [-1.7664],\n",
       "          [-1.7580],\n",
       "          [-1.7496],\n",
       "          [-1.7244],\n",
       "          [-1.7327],\n",
       "          [-1.7917],\n",
       "          [-1.7580],\n",
       "          [-1.8254],\n",
       "          [-1.8002],\n",
       "          [-1.8338],\n",
       "          [-1.8254],\n",
       "          [-1.8338],\n",
       "          [-1.8422],\n",
       "          [-1.7496],\n",
       "          [-1.6991],\n",
       "          [-1.6653],\n",
       "          [-1.5895],\n",
       "          [-1.5811],\n",
       "          [-1.4547],\n",
       "          [-1.5473],\n",
       "          [-1.5389],\n",
       "          [-1.6231],\n",
       "          [-1.6653],\n",
       "          [-1.6906],\n",
       "          [-1.6231],\n",
       "          [-1.6485],\n",
       "          [-1.6231],\n",
       "          [-1.5895],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.6231],\n",
       "          [-1.5895],\n",
       "          [-1.5895],\n",
       "          [-1.5642],\n",
       "          [-1.6064],\n",
       "          [-1.6231],\n",
       "          [-1.6569],\n",
       "          [-1.7075],\n",
       "          [-1.6653],\n",
       "          [-1.5726],\n",
       "          [-1.4968],\n",
       "          [-1.3873],\n",
       "          [-1.4378],\n",
       "          [-1.4378],\n",
       "          [-1.2019],\n",
       "          [-1.4800]]], dtype=torch.float64),\n",
       " tensor([[[-1.9518],\n",
       "          [-1.8760],\n",
       "          [-1.8254],\n",
       "          [-1.8676],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.8844],\n",
       "          [-1.8844],\n",
       "          [-1.8169],\n",
       "          [-1.8422],\n",
       "          [-1.8254],\n",
       "          [-1.7749],\n",
       "          [-1.8591],\n",
       "          [-1.8591],\n",
       "          [-1.7917],\n",
       "          [-1.8676],\n",
       "          [-1.8591],\n",
       "          [-1.8929],\n",
       "          [-1.8169],\n",
       "          [-1.7749],\n",
       "          [-1.7496],\n",
       "          [-1.7580],\n",
       "          [-1.7158],\n",
       "          [-1.7411],\n",
       "          [-1.7749],\n",
       "          [-1.7664],\n",
       "          [-1.7580],\n",
       "          [-1.7496],\n",
       "          [-1.7244],\n",
       "          [-1.7327],\n",
       "          [-1.7917],\n",
       "          [-1.7580],\n",
       "          [-1.8254],\n",
       "          [-1.8002],\n",
       "          [-1.8338],\n",
       "          [-1.8254],\n",
       "          [-1.8338],\n",
       "          [-1.8422],\n",
       "          [-1.7496],\n",
       "          [-1.6991],\n",
       "          [-1.6653],\n",
       "          [-1.5895],\n",
       "          [-1.5811],\n",
       "          [-1.4547],\n",
       "          [-1.5473],\n",
       "          [-1.5389],\n",
       "          [-1.6231],\n",
       "          [-1.6653],\n",
       "          [-1.6906],\n",
       "          [-1.6231],\n",
       "          [-1.6485],\n",
       "          [-1.6231],\n",
       "          [-1.5895],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.7411],\n",
       "          [-1.6231],\n",
       "          [-1.5895],\n",
       "          [-1.5895],\n",
       "          [-1.5642],\n",
       "          [-1.6064],\n",
       "          [-1.6231],\n",
       "          [-1.6569],\n",
       "          [-1.7075],\n",
       "          [-1.6653],\n",
       "          [-1.5726],\n",
       "          [-1.4968],\n",
       "          [-1.3873],\n",
       "          [-1.4378],\n",
       "          [-1.4378],\n",
       "          [-1.2019],\n",
       "          [-1.4800],\n",
       "          [-1.4968],\n",
       "          [-1.5220],\n",
       "          [-1.4968],\n",
       "          [-1.5220],\n",
       "          [-1.5558],\n",
       "          [-1.5137],\n",
       "          [-1.4378],\n",
       "          [-1.4800],\n",
       "          [-1.4968],\n",
       "          [-1.4293],\n",
       "          [-1.3957],\n",
       "          [-1.4462],\n",
       "          [-1.4631],\n",
       "          [-1.4884],\n",
       "          [-1.4800],\n",
       "          [-1.5053],\n",
       "          [-1.4884],\n",
       "          [-1.4209],\n",
       "          [-1.5473],\n",
       "          [-1.4631],\n",
       "          [-1.5220],\n",
       "          [-1.5389],\n",
       "          [-1.5558],\n",
       "          [-1.4884],\n",
       "          [-1.5558],\n",
       "          [-1.5305],\n",
       "          [-1.6738],\n",
       "          [-1.7075],\n",
       "          [-1.7244],\n",
       "          [-1.6569],\n",
       "          [-1.6485],\n",
       "          [-1.7075],\n",
       "          [-1.7075],\n",
       "          [-1.8507],\n",
       "          [-1.8676],\n",
       "          [-1.8676],\n",
       "          [-1.7917],\n",
       "          [-1.8591],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.9518],\n",
       "          [-1.8591],\n",
       "          [-1.7833],\n",
       "          [-1.7075],\n",
       "          [-1.8002],\n",
       "          [-1.7158],\n",
       "          [-1.5389],\n",
       "          [-1.5389],\n",
       "          [-1.5137],\n",
       "          [-1.5642],\n",
       "          [-1.6569],\n",
       "          [-1.6653],\n",
       "          [-1.7496],\n",
       "          [-1.8002],\n",
       "          [-1.6906],\n",
       "          [-1.8591],\n",
       "          [-1.7244],\n",
       "          [-1.7327],\n",
       "          [-1.6822],\n",
       "          [-1.7075],\n",
       "          [-1.6906],\n",
       "          [-1.6653],\n",
       "          [-1.7496],\n",
       "          [-1.7664],\n",
       "          [-1.7244],\n",
       "          [-1.6316],\n",
       "          [-1.6064],\n",
       "          [-1.4968],\n",
       "          [-1.5979],\n",
       "          [-1.5473],\n",
       "          [-1.4209],\n",
       "          [-1.4884],\n",
       "          [-1.4547],\n",
       "          [-1.4547],\n",
       "          [-1.5642],\n",
       "          [-1.6064],\n",
       "          [-1.6569],\n",
       "          [-1.5389],\n",
       "          [-1.4631],\n",
       "          [-1.5220],\n",
       "          [-1.4378],\n",
       "          [-1.4968],\n",
       "          [-1.5726],\n",
       "          [-1.4631],\n",
       "          [-1.5811],\n",
       "          [-1.5220],\n",
       "          [-1.6064],\n",
       "          [-1.5895],\n",
       "          [-1.5220],\n",
       "          [-1.5389],\n",
       "          [-1.6064],\n",
       "          [-1.4884],\n",
       "          [-1.6064],\n",
       "          [-1.5558],\n",
       "          [-1.4126],\n",
       "          [-1.4715]]], dtype=torch.float64),\n",
       " tensor([[[ 0.1957, -0.3333,  0.0000, -0.4589],\n",
       "          [ 0.2391, -0.3333,  0.0000, -0.4589],\n",
       "          [ 0.2826, -0.3333,  0.0000, -0.4589],\n",
       "          ...,\n",
       "          [ 0.0652,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.1087,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.1522,  0.0000, -0.5000, -0.4151]]], dtype=torch.float64),\n",
       " tensor([[[ 0.1957,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.2391,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.2826,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.3261,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.3696,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.4130,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.4565,  0.5000,  0.4000, -0.4260],\n",
       "          [ 0.5000,  0.5000,  0.4000, -0.4260],\n",
       "          [-0.5000, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.4565, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.4130, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.3696, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.3261, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.2826, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.2391, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.1957, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.1522, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.1087, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.0652, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.0217, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.0217, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.0652, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.1087, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.1522, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.1957, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.2391, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.2826, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.3261, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.3696, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.4130, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.4565, -0.5000,  0.4333, -0.4233],\n",
       "          [ 0.5000, -0.5000,  0.4333, -0.4233],\n",
       "          [-0.5000, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.4565, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.4130, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.3696, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.3261, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.2826, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.2391, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.1957, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.1522, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.1087, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.0652, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.0217, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.0217, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.0652, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.1087, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.1522, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.1957, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.2391, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.2826, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.3261, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.3696, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.4130, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.4565, -0.3333,  0.4667, -0.4205],\n",
       "          [ 0.5000, -0.3333,  0.4667, -0.4205],\n",
       "          [-0.5000, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.4565, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.4130, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.3696, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.3261, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.2826, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.2391, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.1957, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.1522, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.1087, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.0652, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.0217, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.0217, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.0652, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.1087, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.1522, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.1957, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.2391, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.2826, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.3261, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.3696, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.4130, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.4565, -0.1667,  0.5000, -0.4178],\n",
       "          [ 0.5000, -0.1667,  0.5000, -0.4178],\n",
       "          [-0.5000,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.4565,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.4130,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.3696,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.3261,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.2826,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.2391,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.1957,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.1522,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.1087,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.0652,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.0217,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.0217,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.0652,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.1087,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.1522,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.1957,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.2391,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.2826,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.3261,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.3696,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.4130,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.4565,  0.0000, -0.5000, -0.4151],\n",
       "          [ 0.5000,  0.0000, -0.5000, -0.4151],\n",
       "          [-0.5000,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.4565,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.4130,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.3696,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.3261,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.2826,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.2391,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.1957,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.1522,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.1087,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.0652,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.0217,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.0217,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.0652,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.1087,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.1522,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.1957,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.2391,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.2826,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.3261,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.3696,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.4130,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.4565,  0.1667, -0.4667, -0.4123],\n",
       "          [ 0.5000,  0.1667, -0.4667, -0.4123],\n",
       "          [-0.5000,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.4565,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.4130,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.3696,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.3261,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.2826,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.2391,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.1957,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.1522,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.1087,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.0652,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.0217,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.0217,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.0652,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.1087,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.1522,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.1957,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.2391,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.2826,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.3261,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.3696,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.4130,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.4565,  0.3333, -0.4333, -0.4096],\n",
       "          [ 0.5000,  0.3333, -0.4333, -0.4096],\n",
       "          [-0.5000,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.4565,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.4130,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.3696,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.3261,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.2826,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.2391,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.1957,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.1522,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.1087,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.0652,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.0217,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.0217,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.0652,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.1087,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.1522,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.1957,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.2391,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.2826,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.3261,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.3696,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.4130,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.4565,  0.5000, -0.4000, -0.4068],\n",
       "          [ 0.5000,  0.5000, -0.4000, -0.4068],\n",
       "          [-0.5000, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.4565, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.4130, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.3696, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.3261, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.2826, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.2391, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.1957, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.1522, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.1087, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.0652, -0.5000, -0.3667, -0.4041],\n",
       "          [-0.0217, -0.5000, -0.3667, -0.4041],\n",
       "          [ 0.0217, -0.5000, -0.3667, -0.4041],\n",
       "          [ 0.0652, -0.5000, -0.3667, -0.4041],\n",
       "          [ 0.1087, -0.5000, -0.3667, -0.4041],\n",
       "          [ 0.1522, -0.5000, -0.3667, -0.4041]]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Comparing performance of ForecastPFN to other Time Series Benchmarks')\n",
    "\n",
    "    parser.add_argument('--is_training', type=int, default=0, help='status')\n",
    "    parser.add_argument('--use_gpu', type=bool, default=True, help='status')\n",
    "    parser.add_argument('--itr', type=int, default=5, help='status')\n",
    "\n",
    "    # model settings\n",
    "    parser.add_argument('--model', type=str, default='ForecastPFN',\n",
    "                        help='model name, options: [ForecastPFN, FEDformer, Autoformer, Informer, Transformer, Arima, Prophet]')\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument('--seq_len', type=int, default=384,\n",
    "                        help='input sequence length')\n",
    "    parser.add_argument('--label_len', type=int,\n",
    "                        default=96, help='start token length')\n",
    "    parser.add_argument('--pred_len', type=int, default=96,\n",
    "                        help='prediction sequence length')\n",
    "\n",
    "    # parser.add_argument('--time_budget', type=int,\n",
    "    #                     help='amount of time budget to train the model')\n",
    "    # parser.add_argument('--train_budget', type=int,\n",
    "    #                     help='length of training sequence')\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument('--data', type=str,\n",
    "                        default='ETTh1', help='dataset type')\n",
    "    parser.add_argument('--root_path', type=str,\n",
    "                        default='../academic_data/ETT-small/', help='root path of the data file')\n",
    "    parser.add_argument('--data_path', type=str,\n",
    "                        default='ETTh1_.csv', help='data file')\n",
    "    parser.add_argument('--target', type=str,\n",
    "                        default='OT', help='name of target column')\n",
    "    parser.add_argument('--scale', type=bool, default=True,\n",
    "                        help='scale the time series with sklearn.StandardScale()')\n",
    "\n",
    "    # ForecastPFN\n",
    "    parser.add_argument('--model_path', type=str, default='../saved_weights/',\n",
    "                        help='encoder input size')\n",
    "    parser.add_argument('--scaler', type=str, default='standard',\n",
    "                        help='scale the test series with sklearn.StandardScale()')\n",
    "\n",
    "    # Metalearn\n",
    "    parser.add_argument('--metalearn_freq', type=str,\n",
    "                        help='which type of model should be used for the Metalearn model. Typically M, W, or D.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--is_training IS_TRAINING]\n",
      "                             [--use_gpu USE_GPU] [--itr ITR] [--model MODEL]\n",
      "                             [--seq_len SEQ_LEN] [--label_len LABEL_LEN]\n",
      "                             [--pred_len PRED_LEN] [--data DATA]\n",
      "                             [--root_path ROOT_PATH] [--data_path DATA_PATH]\n",
      "                             [--target TARGET] [--scale SCALE]\n",
      "                             [--model_path MODEL_PATH] [--scaler SCALER]\n",
      "                             [--metalearn_freq METALEARN_FREQ]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"dc6ad997-58c5-44b8-ac65-6167014f18e9\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=/home/jovyan/.local/share/jupyter/runtime/kernel-v2-347175Fvwj4DSEoA6W.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = parse()\n",
    "args = parser.parse_args()\n",
    "args = resolve_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set, data_loader = data_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = Dataset_Custom(root_path='/home/jovyan/kkuvshinova/ForecastPFN_accelerate/academic_data/ETT-small')\n",
    "a = dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.label_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.pred_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.train_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.70525639],\n",
       "       [1.37657355],\n",
       "       [1.37657355],\n",
       "       [1.04801043],\n",
       "       [0.67716404],\n",
       "       [0.58445245],\n",
       "       [0.77826037],\n",
       "       [0.82042369],\n",
       "       [0.64350517],\n",
       "       [0.13790359],\n",
       "       [0.44131252],\n",
       "       [0.45808198],\n",
       "       [0.34860092],\n",
       "       [0.27277873],\n",
       "       [0.39064452],\n",
       "       [0.12101441],\n",
       "       [0.38225979],\n",
       "       [0.34009648],\n",
       "       [0.52551944],\n",
       "       [0.2895482 ],\n",
       "       [0.18845187],\n",
       "       [0.20534128],\n",
       "       [0.20534128],\n",
       "       [0.41603838],\n",
       "       [0.57606772],\n",
       "       [0.40753393],\n",
       "       [0.44969725],\n",
       "       [0.5339044 ],\n",
       "       [0.60972659],\n",
       "       [0.71094263],\n",
       "       [0.47497139],\n",
       "       [0.57606772],\n",
       "       [0.51713471],\n",
       "       [0.2475046 ],\n",
       "       [0.55917831],\n",
       "       [0.37387506],\n",
       "       [0.54240885],\n",
       "       [0.46658666],\n",
       "       [0.60134186],\n",
       "       [0.68566849],\n",
       "       [0.52551944],\n",
       "       [1.09855848],\n",
       "       [1.14910676],\n",
       "       [1.38495828],\n",
       "       [1.31764054],\n",
       "       [0.71094263],\n",
       "       [0.67716404],\n",
       "       [1.31764054],\n",
       "       [1.56199652],\n",
       "       [1.16587622],\n",
       "       [0.96368356],\n",
       "       [1.2164245 ],\n",
       "       [1.15749149],\n",
       "       [0.93002469],\n",
       "       [1.0816693 ],\n",
       "       [0.87947642],\n",
       "       [1.20803954],\n",
       "       [1.32602527],\n",
       "       [1.41023242],\n",
       "       [1.57876621],\n",
       "       [1.25858782],\n",
       "       [1.63781893],\n",
       "       [1.68836698],\n",
       "       [1.78107858],\n",
       "       [1.61254479],\n",
       "       [1.915834  ],\n",
       "       [1.87379017],\n",
       "       [1.46928515],\n",
       "       [0.62661577],\n",
       "       [0.71094263],\n",
       "       [0.37387506],\n",
       "       [0.27277873],\n",
       "       [0.64350517],\n",
       "       [1.10694321],\n",
       "       [1.38495828],\n",
       "       [1.39346273],\n",
       "       [0.99734244],\n",
       "       [0.90475056],\n",
       "       [0.81203896],\n",
       "       [0.7446015 ],\n",
       "       [0.64350517],\n",
       "       [1.09005403],\n",
       "       [1.06477989],\n",
       "       [1.58727065],\n",
       "       [1.57876621],\n",
       "       [1.55349207],\n",
       "       [1.75580444],\n",
       "       [1.84839632],\n",
       "       [2.0169301 ],\n",
       "       [1.5198332 ],\n",
       "       [1.5198332 ],\n",
       "       [1.84839632],\n",
       "       [1.06477989],\n",
       "       [1.70525639],\n",
       "       [1.35968414],\n",
       "       [1.09855848],\n",
       "       [1.15749149],\n",
       "       [1.15749149],\n",
       "       [1.16587622],\n",
       "       [1.51144847],\n",
       "       [1.69675171],\n",
       "       [1.80635272],\n",
       "       [1.97488673],\n",
       "       [1.84839632],\n",
       "       [1.67998225],\n",
       "       [1.69675171],\n",
       "       [1.99165619],\n",
       "       [1.77257413],\n",
       "       [1.12383262],\n",
       "       [1.30075113],\n",
       "       [1.49455906],\n",
       "       [1.71364112],\n",
       "       [1.88217491],\n",
       "       [2.0169301 ],\n",
       "       [1.82312218],\n",
       "       [1.7641894 ],\n",
       "       [1.81473745],\n",
       "       [1.84839632],\n",
       "       [1.75580444],\n",
       "       [1.69675171],\n",
       "       [1.51144847],\n",
       "       [1.68836698],\n",
       "       [1.82312218],\n",
       "       [1.78946331],\n",
       "       [1.70525639],\n",
       "       [1.65470811],\n",
       "       [1.67998225],\n",
       "       [1.59565539],\n",
       "       [1.70525639],\n",
       "       [2.00016064],\n",
       "       [1.6714778 ],\n",
       "       [1.6714778 ],\n",
       "       [1.72202585],\n",
       "       [1.83162686],\n",
       "       [1.87379017],\n",
       "       [1.5198332 ],\n",
       "       [1.36818859],\n",
       "       [1.72202585],\n",
       "       [0.96368356],\n",
       "       [1.60404035],\n",
       "       [1.6714778 ],\n",
       "       [1.63781893],\n",
       "       [1.58727065],\n",
       "       [1.46090019],\n",
       "       [1.46928515],\n",
       "       [1.49455906],\n",
       "       [1.36818859],\n",
       "       [1.41873687],\n",
       "       [1.32602527],\n",
       "       [1.25020308],\n",
       "       [1.20803954],\n",
       "       [1.09005403],\n",
       "       [1.39346273],\n",
       "       [1.5198332 ],\n",
       "       [1.61254479],\n",
       "       [1.71364112],\n",
       "       [1.95799709],\n",
       "       [2.28667993],\n",
       "       [2.51414696],\n",
       "       [2.6574066 ],\n",
       "       [2.45509423],\n",
       "       [2.13491584],\n",
       "       [2.1012572 ],\n",
       "       [2.03381928],\n",
       "       [1.99165619],\n",
       "       [2.07598283],\n",
       "       [1.95799709],\n",
       "       [1.83162686],\n",
       "       [1.77257413],\n",
       "       [1.73053053],\n",
       "       [1.56199652],\n",
       "       [1.49455906],\n",
       "       [1.5367226 ],\n",
       "       [1.55349207],\n",
       "       [1.57038148],\n",
       "       [1.51144847],\n",
       "       [1.62931426],\n",
       "       [1.70525639],\n",
       "       [1.72202585],\n",
       "       [1.72202585],\n",
       "       [1.77257413],\n",
       "       [1.74729999],\n",
       "       [1.96638183],\n",
       "       [2.21073803],\n",
       "       [2.16869465],\n",
       "       [2.16018975],\n",
       "       [2.07598283],\n",
       "       [2.0675981 ],\n",
       "       [1.97488673],\n",
       "       [1.89906408],\n",
       "       [1.78946331],\n",
       "       [1.70525639],\n",
       "       [1.75580444],\n",
       "       [1.67998225],\n",
       "       [1.57038148],\n",
       "       [1.48617433],\n",
       "       [1.49455906],\n",
       "       [1.51144847],\n",
       "       [1.43550633],\n",
       "       [1.47766988],\n",
       "       [1.49455906],\n",
       "       [1.63781893],\n",
       "       [1.61254479],\n",
       "       [1.67998225],\n",
       "       [1.81473745],\n",
       "       [1.73053053],\n",
       "       [1.84001159],\n",
       "       [1.915834  ],\n",
       "       [1.98327146],\n",
       "       [1.89055964],\n",
       "       [1.86528573],\n",
       "       [1.80635272],\n",
       "       [1.74729999],\n",
       "       [1.74729999],\n",
       "       [1.63781893],\n",
       "       [1.63781893],\n",
       "       [1.57876621],\n",
       "       [1.56199652],\n",
       "       [1.62092952],\n",
       "       [1.5367226 ],\n",
       "       [1.46928515],\n",
       "       [1.47766988],\n",
       "       [1.37657355],\n",
       "       [1.34291468],\n",
       "       [1.41023242],\n",
       "       [1.59565539],\n",
       "       [1.57876621],\n",
       "       [1.52821793],\n",
       "       [1.57876621],\n",
       "       [1.5367226 ],\n",
       "       [1.60404035],\n",
       "       [1.7641894 ],\n",
       "       [1.78107858],\n",
       "       [1.72202585],\n",
       "       [1.63781893],\n",
       "       [1.66309307],\n",
       "       [1.57876621],\n",
       "       [1.57038148],\n",
       "       [1.4271216 ],\n",
       "       [1.5367226 ],\n",
       "       [1.46928515],\n",
       "       [1.54510734],\n",
       "       [1.46928515],\n",
       "       [1.55349207],\n",
       "       [1.52821793],\n",
       "       [1.51144847],\n",
       "       [1.62092952],\n",
       "       [1.62092952],\n",
       "       [1.54510734],\n",
       "       [1.74729999],\n",
       "       [1.78946331],\n",
       "       [1.90744927],\n",
       "       [2.00016064],\n",
       "       [2.10964193],\n",
       "       [2.21073803],\n",
       "       [2.21073803],\n",
       "       [2.21924248],\n",
       "       [2.22762721],\n",
       "       [2.04220447],\n",
       "       [1.84001159],\n",
       "       [2.05909365],\n",
       "       [1.95799709],\n",
       "       [1.94110792],\n",
       "       [2.00854537],\n",
       "       [2.05909365],\n",
       "       [2.08436756],\n",
       "       [2.0675981 ],\n",
       "       [2.04220447],\n",
       "       [2.00016064],\n",
       "       [2.04220447],\n",
       "       [2.10964193],\n",
       "       [2.02543455],\n",
       "       [2.14342029],\n",
       "       [2.35399767],\n",
       "       [2.40454595],\n",
       "       [2.37088731],\n",
       "       [2.37088731],\n",
       "       [2.38777649],\n",
       "       [2.64890215],\n",
       "       [2.7921618 ],\n",
       "       [2.85121452],\n",
       "       [2.9017628 ],\n",
       "       [2.58158441],\n",
       "       [2.52253169],\n",
       "       [2.36250258],\n",
       "       [2.37927204],\n",
       "       [2.36250258],\n",
       "       [2.38777649],\n",
       "       [2.42143513],\n",
       "       [2.4467095 ],\n",
       "       [2.42982032],\n",
       "       [2.56469523],\n",
       "       [2.56469523],\n",
       "       [2.5563105 ],\n",
       "       [2.48048786],\n",
       "       [2.66579133],\n",
       "       [2.60685832],\n",
       "       [2.48048786],\n",
       "       [2.48048786],\n",
       "       [1.62092952],\n",
       "       [1.98327146],\n",
       "       [2.37927204],\n",
       "       [2.33722821],\n",
       "       [2.57307996],\n",
       "       [2.78377707],\n",
       "       [2.63213269],\n",
       "       [2.21073803],\n",
       "       [2.09287201],\n",
       "       [1.98327146],\n",
       "       [2.0169301 ],\n",
       "       [2.16018975],\n",
       "       [2.28667993],\n",
       "       [2.18546412],\n",
       "       [1.96638183],\n",
       "       [2.10964193],\n",
       "       [2.2023533 ],\n",
       "       [2.32033903],\n",
       "       [2.17707939],\n",
       "       [2.27817548],\n",
       "       [2.1012572 ],\n",
       "       [2.34561294],\n",
       "       [2.30344985],\n",
       "       [2.42982032],\n",
       "       [2.52253169],\n",
       "       [2.19396856],\n",
       "       [2.1012572 ],\n",
       "       [2.02543455],\n",
       "       [2.2023533 ],\n",
       "       [2.4467095 ],\n",
       "       [2.21073803],\n",
       "       [1.03112102],\n",
       "       [1.98327146],\n",
       "       [2.0169301 ],\n",
       "       [1.63781893],\n",
       "       [1.68836698],\n",
       "       [1.80635272],\n",
       "       [1.79784827],\n",
       "       [1.95799709],\n",
       "       [2.04220447],\n",
       "       [2.04220447],\n",
       "       [2.19396856],\n",
       "       [2.10964193],\n",
       "       [2.18546412],\n",
       "       [2.28667993],\n",
       "       [2.16018975],\n",
       "       [1.83162686],\n",
       "       [1.80635272],\n",
       "       [1.73053053],\n",
       "       [1.83162686],\n",
       "       [1.80635272],\n",
       "       [1.7641894 ],\n",
       "       [1.14060208],\n",
       "       [1.03112102],\n",
       "       [0.20534128],\n",
       "       [0.73621677],\n",
       "       [0.07046613],\n",
       "       [0.69405345],\n",
       "       [0.94679416],\n",
       "       [1.13221735],\n",
       "       [1.22492895],\n",
       "       [1.27547722],\n",
       "       [1.32602527],\n",
       "       [1.44401101],\n",
       "       [1.41023242],\n",
       "       [1.44401101],\n",
       "       [1.39346273],\n",
       "       [1.36818859],\n",
       "       [1.62931426],\n",
       "       [1.66309307],\n",
       "       [1.81473745],\n",
       "       [2.02543455],\n",
       "       [2.05070892],\n",
       "       [1.96638183],\n",
       "       [1.98327146],\n",
       "       [2.02543455],\n",
       "       [2.12653111],\n",
       "       [2.30344985],\n",
       "       [2.17707939],\n",
       "       [2.3119543 ],\n",
       "       [2.07598283],\n",
       "       [1.89906408],\n",
       "       [1.88217491],\n",
       "       [1.84839632],\n",
       "       [1.74729999]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.65172522, -1.65172522, -1.65172522, ..., -1.65172522,\n",
       "         -1.65172522, -1.65172522]],\n",
       "\n",
       "       [[-1.64493755, -1.64493755, -1.64493755, ..., -1.64493755,\n",
       "         -1.64493755, -1.64493755]],\n",
       "\n",
       "       [[-1.63885194, -1.63885194, -1.63885194, ..., -1.63885194,\n",
       "         -1.63885194, -1.63885194]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.77096854, -0.77096854, -0.77096854, ..., -0.77096854,\n",
       "         -0.77096854, -0.77096854]],\n",
       "\n",
       "       [[-0.77120478, -0.77120478, -0.77120478, ..., -0.77120478,\n",
       "         -0.77120478, -0.77120478]],\n",
       "\n",
       "       [[-0.77096854, -0.77096854, -0.77096854, ..., -0.77096854,\n",
       "         -0.77096854, -0.77096854]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/jovyan/kkuvshinova/ForecastPFN_accelerate/benchmark/results_history/ETTh1/ForecastPFN_ETTh1_sl36_ll18_pl14_timebudget_None_trainbudget_50_model-path_saved_weights_targetOT_itr_0/true.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isinf(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/jovyan/kkuvshinova/ForecastPFN_accelerate/benchmark/results_history/ETTh1/ForecastPFN_ETTh1_sl36_ll18_pl14_timebudget_None_trainbudget_50_model-path_saved_weights_targetOT_itr_0/metrics.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'metrics': {'mae': 0.227779650408567, 'mse': 0.09279294257578616, 'rmse': 0.3046193404493322, 'mape': 0.3487395024324892, 'mspe': 4.013795182933621}, 'train_timer': None, 'vali_timer': None, 'test_timer': 0.09956669807434082, 'args': Namespace(is_training=0, use_gpu=True, itr=5, model='ForecastPFN', seq_len=36, label_len=18, pred_len=14, time_budget=None, train_budget=50, data='ETTh1', root_path='../academic_data/ETT-small/', data_path='ETTh1_.csv', target='OT', scale=True, model_path='../saved_weights/', scaler=StandardScaler(), metalearn_freq=None, features='S', freq='h', checkpoints='./checkpoints/', embed='timeF', batch_size=512, gpu=0, use_multi_gpu=True, devices='0,1', num_workers=10, model_name='saved_weights', dvices='0,1', device_ids=[0, 1])},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/jovyan/kkuvshinova/ForecastPFN_accelerate/benchmark/results_history/ETTh1/ForecastPFN_ETTh1_sl36_ll18_pl96_timebudget_None_trainbudget_1_model-path_.._targetHUFL_itr_0/pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.75126138,  0.75126138,  0.75126138, ...,  0.75126138,\n",
       "          0.75126138,  0.75126138]],\n",
       "\n",
       "       [[ 0.7011685 ,  0.7011685 ,  0.7011685 , ...,  0.7011685 ,\n",
       "          0.7011685 ,  0.7011685 ]],\n",
       "\n",
       "       [[ 0.69326071,  0.69326071,  0.69326071, ...,  0.69326071,\n",
       "          0.69326071,  0.69326071]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.4656374 , -0.4656374 , -0.4656374 , ..., -0.4656374 ,\n",
       "         -0.4656374 , -0.4656374 ]],\n",
       "\n",
       "       [[-0.4999147 , -0.4999147 , -0.4999147 , ..., -0.4999147 ,\n",
       "         -0.4999147 , -0.4999147 ]],\n",
       "\n",
       "       [[-0.51573465, -0.51573465, -0.51573465, ..., -0.51573465,\n",
       "         -0.51573465, -0.51573465]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/jovyan/kkuvshinova/ForecastPFN_accelerate/benchmark/results_history/ETTh1/ForecastPFN_ETTh1_sl36_ll18_pl96_timebudget_None_trainbudget_1_model-path_.._targetHUFL_itr_0/true.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.5921752 ],\n",
       "        [ 0.9264881 ],\n",
       "        [ 1.2850153 ],\n",
       "        ...,\n",
       "        [-3.3131723 ],\n",
       "        [-2.9546452 ],\n",
       "        [-2.3007302 ]],\n",
       "\n",
       "       [[ 0.9264881 ],\n",
       "        [ 1.2850153 ],\n",
       "        [ 1.7701373 ],\n",
       "        ...,\n",
       "        [-2.9546452 ],\n",
       "        [-2.3007302 ],\n",
       "        [-1.1616936 ]],\n",
       "\n",
       "       [[ 1.2850153 ],\n",
       "        [ 1.7701373 ],\n",
       "        [ 1.6647992 ],\n",
       "        ...,\n",
       "        [-2.3007302 ],\n",
       "        [-1.1616936 ],\n",
       "        [ 0.8104429 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.27584606],\n",
       "        [-0.20199916],\n",
       "        [-0.13870184],\n",
       "        ...,\n",
       "        [-1.4358245 ],\n",
       "        [-2.0369916 ],\n",
       "        [-0.72931933]],\n",
       "\n",
       "       [[-0.20199916],\n",
       "        [-0.13870184],\n",
       "        [-0.13870184],\n",
       "        ...,\n",
       "        [-2.0369916 ],\n",
       "        [-0.72931933],\n",
       "        [ 0.28312278]],\n",
       "\n",
       "       [[-0.13870184],\n",
       "        [-0.13870184],\n",
       "        [ 0.25147408],\n",
       "        ...,\n",
       "        [-0.72931933],\n",
       "        [ 0.28312278],\n",
       "        [ 0.42026702]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpfn",
   "language": "python",
   "name": "fpfn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
